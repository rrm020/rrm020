---
title: "Takehome Midterm"
author: "Riley McDonnell"
date: "23 Mar 2020"
output:
  github_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

For the takehome midterm you will have 48 hours to complete the objectives listed below. The deadline for submission is 18 March 2020 at 2pm EST.

Date/Time started: Sunday March 23rd 11:30 am
Date/Time completed: Sunday March 23rd 

You will be graded on the following criteria:

* Completion of the objectives
* Successful knitting of the pdf
* Readability (tidyness) of Rmd code
* Acknowledgement of resources

## Loading Libraries

Load all of your libraries in this code block. Indicate why each library is necessary.

```{r Load Libraries, include=FALSE}
if (!require("MASS")) install.packages("MASS"); library(MASS)
# MASS is needed for the negative binomial glm function glm.nb()

if (!require("tidyverse")) install.packages("tidyverse"); library(tidyverse)
# tidyverse is needed for ggplot2, readr, and dplyr
# Always load tidyverse last so that its functions will not get masked by other packages
# It is also good practice to specify dplyr::filter() and dplyr::select() functions

if (!require("UsingR")) install.packages("UsingR")
library(UsingR)
# need for data exploration and distribution testing

if (!require("githubinstall")) install.packages("githubinstall")
library(githubinstall)
# need in order to use githubinstall in next package loading

if (!require("ggcorrplot")) githubinstall("kassambara/ggcorrplot")
library(ggcorrplot)
# need to visualize correlations
```

## Objectives for Midterm Exam

* [X] Import, clean, merge data tables
* [ ] Present graphical summary of Dengue incidence data
* [ ] Data exploration of potential explanatory variables
* [ ] Test Benchmark model of Dengue incidence
* [ ] Improve model of Dengue incidence

## Background

This dataset should be familiar from Lab 6. We will be using the Dengue dataset from a Driven Data competition: 
https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/

The data for this competition comes from multiple sources aimed at supporting the Predict the Next Pandemic Initiative (https://www.whitehouse.gov/blog/2015/06/05/back-future-using-historical-dengue-data-predict-next-epidemic). 
Dengue surveillance data is provided by the U.S. Centers for Disease Control and prevention, as well as the Department of Defense's Naval Medical Research Unit 6 and the Armed Forces Health Surveillance Center, in collaboration with the Peruvian government and U.S. universities. 
Environmental and climate data is provided by the National Oceanic and Atmospheric Administration (NOAA), an agency of the U.S. Department of Commerce.

The data is provided in two separate files:

1. dengue_features_train: weekly weather and vegetation data for two cities
2. dengue_labels_train: weekly number of dengue cases in each city

There are two cities, San Juan, Puerto Rico and Iquitos, Peru, with test data for each city spanning 5 and 3 years respectively. The data for each city have been concatenated along with a city column indicating the source: *sj* for San Juan and *iq* for Iquitos. 

```{r Read Data}
dengue_features_train <- read_csv("https://s3.amazonaws.com/drivendata/data/44/public/dengue_features_train.csv")
dengue_labels_train <- read_csv("https://s3.amazonaws.com/drivendata/data/44/public/dengue_labels_train.csv")
```

## Feature Descriptions

You are provided the following set of information on a (year, weekofyear) timescale:

(Where appropriate, units are provided as a _unit suffix on the feature name.)

City and date indicators

- city – City abbreviations: sj for San Juan and iq for Iquitos
- week_start_date – Date given in yyyy-mm-dd format

NOAA's GHCN daily climate data weather station measurements

- station_max_temp_c – Maximum temperature
- station_min_temp_c – Minimum temperature
- station_avg_temp_c – Average temperature
- station_precip_mm – Total precipitation
- station_diur_temp_rng_c – Diurnal temperature range

PERSIANN satellite precipitation measurements (0.25x0.25 degree scale)

- precipitation_amt_mm – Total precipitation

NOAA's NCEP Climate Forecast System Reanalysis measurements (0.5x0.5 degree scale)

- reanalysis_sat_precip_amt_mm – Total precipitation
- reanalysis_dew_point_temp_k – Mean dew point temperature
- reanalysis_air_temp_k – Mean air temperature
- reanalysis_relative_humidity_percent – Mean relative humidity
- reanalysis_specific_humidity_g_per_kg – Mean specific humidity
- reanalysis_precip_amt_kg_per_m2 – Total precipitation
- reanalysis_max_air_temp_k – Maximum air temperature
- reanalysis_min_air_temp_k – Minimum air temperature
- reanalysis_avg_temp_k – Average air temperature
- reanalysis_tdtr_k – Diurnal temperature range

Satellite vegetation - Normalized difference vegetation index (NDVI) - NOAA's CDR Normalized Difference Vegetation Index (0.5x0.5 degree scale) measurements

- ndvi_se – Pixel southeast of city centroid
- ndvi_sw – Pixel southwest of city centroid
- ndvi_ne – Pixel northeast of city centroid
- ndvi_nw – Pixel northwest of city centroid

# Takehome Exam Begins here

As a reminder, you may consult your previous homework and group projects, textbook and other readings, and online resources.
Online resources may be used to research ways to solve each problem, but you may not pose questions in online forums about the specific assignment.
You may consult with Prof. Field or with other classmates about technical problems (e.g. where to find a file), but not about how to answer any of the questions.

## (1) Data Wrangling

Use this section to manipulate the two data frames.
1. Follow the Exploratory Data Analysis Checklist (below) to verify the imported data
    a. Check that each variable is the appropriate data class and has values that makes sense
    b. For external verification, at a minimum, check that the annual Dengue incidence numbers for each city are realistic
2. Merge the two data frames, verifying that no information was lost during the merge
3. Check the data for NAs both before and after the merge (note that eliminating all rows or columns with NAs will have consequences)

### Check the packaging

### Run str()

### Look at the top and the bottom of your data

### Check your “n”s

### Validate with at least one external data source

### Merging the features and labels data frames

Although there are dplyr functions for data frame merging, the base `merge()` function is easier to use.

### Dealing with the NAs

Check out the `tidyr::fill()` function for one way to take care of NAs.

```{r data wrangling}
str(dengue_features_train)
str(dengue_labels_train)

nrow(dengue_features_train)
nrow(dengue_labels_train)

summary(dengue_features_train)
summary(dengue_labels_train)


head(dengue_features_train)
tail(dengue_features_train)


head(dengue_labels_train)
tail(dengue_labels_train)


dengue_features_train %>% dplyr::filter(city=="sj") %>% nrow()
dengue_labels_train %>% dplyr::filter(city=="sj") %>% nrow()
dengue_features_train %>% dplyr::filter(city=="iq") %>% nrow()
dengue_labels_train %>% dplyr::filter(city=="iq") %>% nrow()
```
After taking a look at the data for the features and the labels there are some similarities and differences between the two. City is character type for both data sets, but the week start date in the features data set is date type. Other than that exception all other variables are numerical and are quantitative. The city names are "sj" and "iq" which make the data a bit easier to sort and filter, which is what I did at the end of my code above using the helpful package dplyr. As for checking the data, the features data frame has 24 variables and the labels data frame has 4 variables. Both data frames have 1456 rows/observations. Both data frames also have the variables city, year, and weekofyear. By looking at the beginning and end of the data, at least one NA is visible so I will need to remove NA's in future steps. 

To continue with my analysis I will first need to confirm that this data is consistent with other sources. With San Juan, Puerto Rico and Iquitos, Peru being our two cities I looked for papers about dengue fever reports in both locations. The first paper was published in 2015 and looks at cases in Puerto Rico over the recent epidemics in 2007, 2010, and 2012-13. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4587385/
Because this deals with cases by year I will want to look at how San Juan compares by year to the values within the paper.

```{r checking sj}
dengue_labels_train %>%
 dplyr::filter(city=="sj") %>%
 group_by(year) %>%
 summarise(cases = sum(total_cases))
```
The paper's general message is that the values in Puerto Rico have been worsening over time and have years where the number of cases vary. The values in the paper are significantly higher because they represent that of an entire country. The data set we are analyzing is consistent with more recent studies and I think it is safe to proceed with the San Juan values.

Next I will look at a paper written about dengue fever in Iquitos, Peru that studied the long term effects of dengue fever as well as its seasonality. They have data from 2000 to 2010 which lists the cases and strain, as well as the total number of cases per year. I will need to compare the cases per year from our data set.
https://journals.plos.org/plosntds/article?id=10.1371/journal.pntd.0003003

```{r checking iq}
dengue_labels_train %>%
dplyr::filter(city=="iq") %>%
group_by(year) %>%
summarise(cases = sum(total_cases))
```
The values are pretty close, but not 100% exact. Most years are pretty close but 2001 is off by about 300 based on the paper's numbers. Due to the fact that most values are similar I will proceed. 

Next I need to remove the NAs to clean the data. First I need to check to see how many varaibles are impacted by NAs.

```{r check NAs}

apply(dengue_features_train, 2, function(x) any(is.na(x)))
apply(dengue_labels_train, 2, function(x) any(is.na(x)))

```
Because it will be difficult to go between both data sets when the information is helpful for both cities it might be best to merge the data sets.
```{r merge}
data <- merge(dengue_features_train, dengue_labels_train)
length(data)
nrow(data)
```
Because 3 of the varaibles were the same in the data sets they were merged to make 25 variables with 1456 observations, which is exactly the number of observations from both sets so nothing went missing.
```{r checking new data frame}
str(data)
```
The data still looks the same so the merge worked. Next we need to remove the NAs from this new data set. First we need to see what NAs are still present and for what variables.

```{r NAs}
apply(data, 2, function(x) any(is.na(x)))
```
Because we want to properly sort this data we need to sort by variables that currently don't have any NAs, so I'll group by city and arrange by week_start_date by using the fill code that was proposed in the instructions.

```{r remove NAs}
clean.data <- data %>%
 group_by(city) %>%
 arrange(week_start_date) %>%
 tidyr::fill(ndvi_ne, ndvi_nw, ndvi_se, ndvi_sw,
             precipitation_amt_mm,
             reanalysis_air_temp_k,
             reanalysis_avg_temp_k,
             reanalysis_dew_point_temp_k,
             reanalysis_max_air_temp_k,
             reanalysis_min_air_temp_k,
             reanalysis_precip_amt_kg_per_m2,
             reanalysis_relative_humidity_percent,
             reanalysis_sat_precip_amt_mm,
             reanalysis_specific_humidity_g_per_kg,
             reanalysis_tdtr_k,
             station_avg_temp_c,
             station_diur_temp_rng_c,
             station_max_temp_c,
             station_min_temp_c,
             station_precip_mm,
             .direction = "up") 
```
Now time to see if it removed the NAs.

```{r checking NAs}
apply(clean.data, 2, function(x) any(is.na(x)))
```
All false which is a great thing to see and we can proceed to the next step! Just to confirm that we didn't lose anything from the start of this process to the very end.

```{r checking}
summary(dengue_features_train)
summary(dengue_labels_train)
summary(clean.data)
```
Values still look good and we've successfully removed the NAs, time to proceed.

## (2) What is the average number of cases of Dengue for each week of the year for each city?

Provide a publication-quality graphic to present this comparison. 
The graph should span a single year, with the average incidence for each week of the year. 
You are encouraged to explore options, but only your final graph in this section will be used to evaluate this objective.
Consider the most effective way to illustrate any trends or important comparisons within the data.

```{r trying options}
clean.data %>%
 ggplot() +
 aes(weekofyear, total_cases, group = weekofyear, color = city) +
 geom_point() +
 theme_classic()

# this graph makes it clear which city is which and the number of cases per week of the year, although it might be more helpful to look just at the means because those values will be better for future comparisons


clean.data %>%
 group_by(city, weekofyear) %>%
 mutate(average_cases = mean(total_cases)) %>%
 ggplot() +
 aes(weekofyear, average_cases, group = weekofyear, color = city) +
 geom_point() +
 theme_classic()

# this looks a lot better and is much easier to read since you can clearly see each city's results, although this is helpful were going to need to use some error bars to make this more accurate and informative.


plot.data <- clean.data %>%
 group_by(city, weekofyear) %>%
 summarise(average_cases = mean(total_cases),
           sd_cases = sd(total_cases),
           n = length(total_cases)) %>%
 mutate(se = sd_cases/(sqrt(n))) %>%
 mutate(upper = average_cases+se) %>%
 mutate(lower = average_cases-se)


plot.data %>%
 ggplot() +
 aes(x = weekofyear,
     y = average_cases,
     ymin = lower,
     ymax = upper,
     group = weekofyear,
     color = city) +
 geom_pointrange() +
 theme_classic()

# this graph is really helpful and answers the question, but it needs to be cleaned up for a final figure that fully answers the question.

plot.data %>%
 ggplot() +
 geom_linerange(aes(x = weekofyear,
     ymin = lower,
     ymax = upper,
     group = weekofyear,
     color = city),
     alpha = 0.5) +
 geom_point(aes(x = weekofyear,
     y = average_cases,
     group = weekofyear,
     color = city),
     size = 2.5) +
 scale_x_continuous(breaks = seq(0,53,5)) +
 theme_classic() +
 theme(legend.position = "bottom") +
 xlab("Week of the Year") +
 ylab("Average Dengue Fever Cases")

# this is the best version of this graph because it spread out ending at 53 weeks, and is more spread out so the points aren't on top on each other. Everything here is labeled correctly and is easy to understand. This is my final graph that I believe accomplishes the goal for #2.

```
## (3) Data exploration of potential explanatory variables

Consider whether transforming any of the variables might increase the statistical power available.
Explore the correlation of the potential explanatory variables with each other and with dengue incidence.
Present a two or more publication-quality graphics to illustrate your most important findings.

To begin exploring the possible explantory variables I will start by looking at precipitation and temperature which are both interesting points that would help us understand dengue fever in the areas. 
To start I will conduct normality tests to see where the variables stand.
```{r normality tests for precip and temp}
shapiro.test(clean.data$station_avg_temp_c)
simple.eda(clean.data$station_avg_temp_c)
shapiro.test(clean.data$station_precip_mm)
simple.eda(clean.data$station_precip_mm)
```
Both temperature and precipitation have a non-normal distributions, as indicated by the Shapiro test. Temperature is left-skewed while precipitation is highly right-skewed.
```{r normality tests precip and temp transformed}
shapiro.test(log(clean.data$station_avg_temp_c))
simple.eda(log(clean.data$station_avg_temp_c))
shapiro.test(sqrt(clean.data$station_precip_mm))
simple.eda(sqrt(clean.data$station_precip_mm))
```
The QQ plots here are looking much better, but based on the Shapiro tests the data is still not normally distributed. I will proceed with caution as I continue to use the transformed data to begin statistical analyses. 
```{r values for temp and precip}
mean.temp <- log(clean.data$station_avg_temp_c)
mean.precip <- sqrt(clean.data$station_precip_mm)
```

```{r Scatterplot Temp and Precip}
ggplot() + ggtitle("Mean Temperature by Mean Precipiation") +
 aes(x = mean.precip, y = mean.temp) +
 geom_point() + 
  theme_bw() + scale_x_continuous(name = "Square Root of Mean Precipitation") + 
  scale_y_continuous(name = "Natural Log of Mean Temperature") +
 geom_smooth(method=lm , color="black", se=TRUE) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

```{r Statistics on Precip and Temp}
temp.precip <- lm(mean.precip ~ mean.temp)
summary(temp.precip)
```


```{r Cases by Temp}
summary(clean.data$total_cases)
ggplot() + ggtitle("Number of Cases by Mean Temperature") +
 aes(x = mean.temp, y = clean.data$total_cases) +
 geom_point() + theme_bw() + scale_x_continuous(name = "Natural Log of Mean Temperature") + scale_y_continuous(name = "Total Cases") + coord_cartesian(ylim = c(0, 62.5)) +
 geom_smooth(method=lm , color="black", se=TRUE) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```


```{r Statistics on Temp and Cases}
temp.case <- lm(mean.temp ~ clean.data$total_cases)
summary(temp.case)
cor(mean.temp, clean.data$total_cases, method = c("spearman"), use = "complete.obs")
cor(mean.temp, clean.data$total_cases, method = c("pearson"), use = "complete.obs")
```


```{r Cases by Precipitation}
ggplot() + ggtitle("Number of Cases by Mean Precipitation") +
 aes(x = mean.precip, y = clean.data$total_cases) +
 geom_point() + theme_bw() + scale_x_continuous(name = "Square Root of Mean Precipitation") + scale_y_continuous(name = "Total Cases") + coord_cartesian(ylim = c(0, 62.5)) +
 geom_smooth(method=lm , color="black", se=TRUE) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```


```{r Statistics on Precipitation and Cases}
precip.case <- lm(mean.precip ~ clean.data$total_cases)
summary(precip.case)
cor(mean.precip, clean.data$total_cases, method = c("spearman"), use = "complete.obs")
```



### Exploration

Explore the data here.

### Presentation

Present your most important results here.

## (4) Dengue incidence model

Use a generalized linear model to determine the best model for the weekly incidence of Dengue.
At a first pass consider the "Benchmark" model described here: https://shaulab.github.io/DrivenData/DengAI/Benchmark.html
This model is calculated separately for San Jose and Iquitos and only uses the following variables:
 - reanalysis_specific_humidity_g_per_kg
 - reanalysis_dew_point_temp_k 
 - station_avg_temp_c
 - station_min_temp_c
The code for the Benchmark model uses a machine learning approach to optimize the model.
You should use the model selection approach that we have used in BIOL 364, instead.
The total_cases outcome variable is a count - statistically it is a binomial variable that has been summed up over a period of time (a week, in this case). 
Generalized linear models should use a negative binomial distribution (as opposed to a Gaussian distribution, which is what `glm()` assumes) for this type of data.
To fit a negative binomial distribution use `glm.nb()` from the package `MASS` instead of the `glm()` function from `stats`.
 
## (5) Extend the Benchmark model

Consider and test the inclusion of additional explanatory variables to improve the Benchmark model. 

# Acknowledgements

Cite online sources used.
